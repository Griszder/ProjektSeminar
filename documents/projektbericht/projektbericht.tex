\documentclass[paper=A4,pagesize=auto,12pt,headinclude=true,footinclude=true,BCOR=0mm,DIV=calc]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{setspace}
\usepackage[left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}
\onehalfspacing


%opening
\title{Neuronale Netze}
\author{Alexandra Zarkh, Sui Yin Zhang,\\ Lennart Leggewie, Alexander Schallenberg}

\makeatletter
\def\@maketitle{%
	\newpage
	\null
	\vskip 2em%
	\begin{center}%
		\let \footnote \thanks
		{\Huge \textbf{\@title} \par}%
		\vskip 1em%
		{\Large \textbf{Projektbericht}\par}%
		\vskip 2em%
		{\large
			\lineskip .5em%
			\begin{tabular}[t]{c}%
				\@author
			\end{tabular}\par}%
		\vskip 2em%
		{\large Hochschule Bonn-Rhein-Sieg\par}%
		\vskip 2em%
		{\large \@date}%
	\end{center}%
	\par
	\vskip 1.5em}
\makeatother




\begin{document}

\begin{titlepage}
	\maketitle
\end{titlepage}

\tableofcontents
\newpage



\section{Einleitung}
Ein künstliches neuronales Netz (knN) besteht aus vielen kleinen Verarbeitungseinheiten, die durch Gewichtungen miteinander kommunizieren, wie definiert durch Abschnitt 2.1 in \hyperlink{[1]}{“An introduction to Neural Networks” ´[1]}.

\subsection{Aufgabenstellung \& Zielsetzung}
Die Aufgabenstellung lautete zunächst, die Grundlagen eines künstlichen neuronalen Netzes (knN) in Java zu implementieren, sodass mit diesem schon zu einfachen Eingaben eine korrekte Ausgabe kalkuliert wird (Forward Propagation). Diese weiteten sich darauf aus, das Netz trainieren zu können (Backpropagation) und die trainierten Einstellungen des Netzes zu speichern und zu laden.\\
Das Ziel war, die erste Hälfte der Aufgabenstellung in den ersten zwei Wochen und die zweite Hälfte in den folgenden zwei Wochen umzusetzen.

\subsection{Aufgabenkontext \& externe Vorgaben}
Vorgegeben war, ein knN erstellen zu können, dem man bei seiner Erstellung Gewichtungen sowie Biases übergeben kann. Außerdem soll das Netz Ausgaben abhängig von den Eingaben berechnen können und die Gewichtungen und Biases sollen trainiert werden können. Außerdem ist ein Format zur Abspeicherung der Gewichte und Biases vorgegeben worden, welche in diesem Format in eine CSV-Datei gespeichert werden soll.

\subsection{Nicht vorgegeben aber notwendigerweise von uns festgelegt}
Nicht vorgegeben, aber notwendigerweise festgelegt wurde, dass beim Erstellen eines knN die Anzahl der Neuronen für jede Neuronenschicht vom Benutzer festgelegt wird. Implizit wird damit auch die Anzahl der versteckten Schichten verlangt. Außerdem ist die Struktur des Netzes frei gewählt.
% ggf. functions

\subsection{Literaturarbeit: Verweis auf Vorarbeiten}
Die wissenschaftliche Arbeit von Kröse und van der Smagt \hyperlink{1}{[1]}, welches den Aufbau eines knN beschreibt, sowie Erweiterungen dessen, dient der Definition, dem Grundverständnis und dem groben Aufbau eines solchen Netzes.
Das mathematische Verständnis und die darauf aufbauende Implementierung des knN ergab sich aus den Videos 
der Playlist von 3Blue1Brown \hyperlink{2}{[2]}.

\newpage


\section{Methoden}
Die Klassenstruktur des Programmes besteht aus den Klassen \textit{Neuron}, \textit{Network} sowie \textit{NetworkHelper} und einer zusätzlichen Utility-Klasse \textit{Util}.

\subsection{Util}
Die Klasse \textit{Util} bietet drei abstrakte Hilfsmethoden. \textit{random(int)} gibt ein beliebig-dimensionales Array mit zufälligen Zahlen zurück. \textit{addToVec1(double[], double[])} addiert einen Vektor auf den anderen. Die dritte Methode \textit{mulToVec(double, double[])} multipliziert ein Skalar auf einen Vektor. Dies erleichtert das Kreieren und den Umgang mit Vektoren bzw. Arrays im knN.


\subsection{Klassenstruktur Network \& Neuron} %Sui Yin
Im \textit{Neuron} sind die \textit{weights}, der \textit{bias}, die Aktivierungsfunktion sowie deren Ableitung und eine Hilfsvariable \textit{z} als private Attribute definiert.
Es befinden sich zwei Konstruktoren in der Klasse, wobei der Erste den Zweiten mit zufälligen \textit{weights} und einem \textit{bias}=0 sowie der Funktion und deren Ableitung aufruft und der Zweite dafür zuständig ist, einen genau definiertes Neuron zu kreieren.

In der Klasse \textit{Network} wird das knN erstellt.
Es befinden sich zwei Konstanten in der Klasse, die die mathematischen Formeln für die Sigmoidfunktion und deren Ableitung darstellt. Außerdem gibt es noch vier private Attribute,  die in den Methoden mehrfach benutzt werden. Das erste private Attribut “inLayerLength” definiert die Länge der input layer. Die “outputLayer” besteht aus einem eindimensionalen \textit{Neuron}-Array. Dann gibt es noch das Attribut “hiddenLayers”, welches ein zweidimensionales \textit{Neuron}-Array ist, wo der erste Index die Position der jeweiligen Hiddenlayer und der zweite die jeweilige Position des \textit{Neuron}s angibt. Zuletzt gibt es das Attribut “trainable”, welches einen Wahrheitswert ist und standardmäßig wahr ist.

Die \textit{NetworkHelper} Klasse ist dafür da, um Netzwerke zu speichern und zu laden.


\subsection{Konstruktoren \& Initialisierung}
In der Klasse Network gibt es insgesamt 7 Konstruktoren.
Alle außer dem ersten Konstruktor rufen eine “init”-Methode auf um redundanten Code zu verhindern. Die "init"-Methode hat vier Parameter: \textit{int}, \textit{int}, \textit{BiFunction} und ein \textit{int[]}. Die \textit{BiFunction} wird dazu verwendet, der Methode mitzuteilen, wie die Neuronen erstellt werden sollen.
Zuerst werden alle Attribute außer “trainable” entweder direkt mit den passenden Werten oder durch erstellen eines Arrays passender Größe initialisiert.
Darauf folgen zwei Iterationen um diese Arrays mit den übergebenen Werten zu initialisieren.

Der erste Konstruktor ist ein privater leerer Konstruktor für das Laden eines Netzes. 

Der zweite Konstruktor ist dafür da, um ein Netzwerk zu erstellen, wo die Attribute “numInUnit” (die Anzahl der Input Units),”numOutUnit”(die Anzahl der Output Units,“weights”,”biases”, sowie die Anzahl der Hiddenlayers vorgegeben sind.
-> kann gar nicht sein, dass es nicht trainierbar ist, da es default function immer ableitbar ist.

Danach folgt der gleiche Konstruktor, mit dem Unterschied, dass die “weights” und die “biases” nicht angegeben sind.

In den nächsten Konstruktoren, die Aktivierungsfunktionen und deren Ableitungen übergeben bekommen, wird immer erst überprüft, ob das knN trainierbar ist, also ob die Ableitungsfunktion nicht null ist. Falls sie es doch ist, wird dies zwar zugelassen, führt aber dazu, dass das Netz nicht trainierbar ist. 

Der vierte und fünfte Konstruktor haben einen ähnlichen Aufbau von den Parametern wie der zweite und dritte Konstruktor, mit dem Unterschied, dass diesmal eine Funktion, mit dessen Ableitung übergeben wird.

Statt allen Neuronen die gleiche Aktivierungsfunktion und Ableitung zu geben, bekommen die Neuronen in den letzten beiden Konstruktoren jeweils Eigene.


\subsection{Kalkulation}
Die Methode \textit{compute(double[])} nimmt einen Input-Vektor und gibt einen Output-Vektor zurück. Sie berechnet die Ergebnisse aller Layers mit Hilfe der Methode \textit{forwardPropagation(double[])} und gibt den letzten Ergebnisvektor (Output-Vektor) zurück.

\hypertarget{forwardprop}{\subsubsection{Forward Propagation}}
Die Methode \textit{forwardPropagation(double[])} ist als \textit{private} deklariert und gibt ein zweidimensionales Array des Datentyps \textit{double} zurück. Als Parameter bekommt die Methode einen Input-Vektor.\\
Zuerst wird die tatsächliche Länge des Input-Vektors mit der \textit{inLayerLength} verglichen. Wenn die 
beiden unterschiedlich groß sein sollten, dann wird ein Fehler geworfen. Sind die beiden 
verglichenen Werte jedoch gleich groß, wird ein zweidimensionales Ergebnis-Array angelegt, welches
\textit{results} heißt und ebenfalls vom Datentyp \textit{double} ist. Gleichzeitig wird die
Dimension des Arrays auf die Anzahl der "Layers" (Anzahl \textit{hiddenLayers} + 2) gesetzt. Im ersten Eintrag der \textit{results} wird der Input-Vektor gespeichert. 
Für die nächsten Zeilen des Codes muss vorerst eine weitere Methode erklärt werden. Diese heißt 
\textit{fire(double[])} und ist eine öffentliche Methode die einen \textit{double}-Wert zurückgibt. Als  Eingabeparameter wird ein Input-Vektor übergeben. Zu Anfang der Methode wird eine \textit{double}-Variable \textit{sum} angelegt, die das Ergebnis zwischenspeichert. Der Wert einer Variablen \textit{sum} wird durch die Sigma-Regel
\begin{equation}
	sum = \sum_{i=0}^{n} (w_{i} * in_{i}) + \theta
\end{equation}
\hyperlink[1]{[1]} berechnet mit \textit{in} = Input-Vektor, \textit{w} = weights-Vektor, \textit{$\theta$} = \textit{bias} und \textit{n} = Länge von \textit{in}. Als Rückgabewert wird die Aktivierung durch die Summe eingesetzt in die Aktivierungsfunktion \(function.apply(sum)\) zurückgegeben.\\
Die nun folgende \textit{for}-Schleife durchläuft die \textit{hiddenLayers} und berechnet mit Hilfe von \textit{fire(double[])} das Ergebnisarray. Die zweite Dimension des Arrays \textit{results} wird unter Verwendung des \textit{hiddenLayers}-Arrays an der Stelle \textit{i} festgelegt. \textit{results} bekommt dann an der Stelle \textit{i+1} ein neues Array der vorher bestimmten Dimension zugewiesen. Nun folgt eine weitere \textit{for}-Schleife mit dem Index \textit{j}, der die zweite Dimension des Arrays \textit{results} mit  \textit{fire(double[])} berechnet. Dabei wird der Fall überprüft, dass wenn \textit{i} gleich \textit{0} sein sollte, der Input-Vektor als erster Eintrag genommen wird und ansonsten, \textit{results} an der Stelle \textit{i} berechnet wird. 
Zuletzt wird die zweite Dimension beim letzten Eintrag der ersten Dimension auf die Länge der
\textit{outputLayer} festgelegt und die Werte der \textit{outputLayer} mit \textit{fire(double[])} berechnet und in \textit{results} abgespeichert. 
Am Ende wird \textit{results} zurückgegeben.


\subsection{Training}
Die Methode \textit{train(double[][], double, double[][], int)} in der Klasse \textit{Network} bietet dem Benutzer die Möglichkeit das knN bzw. dessen \textit{weights} und \textit{biases} zu trainieren, sodass das Netz nach ausreichend Training immer das gewünschte Ergebnis errechnet. Dazu nimmt die Methode mehrere Input-Vektoren und passende Zielvektoren, sowie eine Lernrate und die Anzahl der Wiederholungen des Trainingsprozesses. Auf die Input-Vektoren wird erst die \hyperlink{forwardprop}{Forward Propagation} angewendet um die Ergebnisse der Neuronen zu erhalten, welche dann für den eigentlichen Lernprozess durch die \hyperlink{backprop}{Backpropagation} benutzt werden. Die Kosten des Durchlaufs werden für einen Trainingsdurchlauf summiert und danach zur Trainingskontrolle ausgegeben. Die Methode \textit{cost(double[], double[])} errechnet diese durch
\begin{equation}
	C_0 = \sum_{i=0}^{n} (o_{i} - y_{i})^2
\end{equation}
\hyperlink{2}{[2]} mit o und y = tatsächliche und gewünschte Ergebnisse und n = Anzahl der Ergebnisse in o.


\hypertarget{backprop}{\subsubsection{Backpropagation}}
Die Umsetzung der Backpropagation ist an die Mathematik von 3Blue1Brown \hyperlink{2}{[2]} angelehnt und beruht auf der Delta-Regel. Zunächst wird eine \textit{LinkedList} angelegt, die der Speicherung der Deltas für die aktuelle Ebene, sowie für die vorherige Ebene dient. Diese Liste hat dadurch immer maximal zwei Elemente. Die Idee ist, das die "neuen" Deltas beim Löschen der "aktuellen" Deltas zu diesen werden.\\Danach werden die ersten Deltas durch die Ableitung der Kostenfunktion ohne Sigma initialisiert. Im nächsten Schritt wird die Backpropagation auf alle Neuronen (innere Schleife) jeder Layer (äußere Schleife), beginnend bei der Output-Layer, angewendet, indem die \textit{backpropagation(double, double, double[])} des jeweiligen Neurons aufgerufen wird und die daraus resultierenden Deltas auf die "neuen" Deltas addiert werden.\\Im Backpropgationteil des Neurons wird jedes Gewicht um den Wert der nach dem Gewicht abgeleiteten Kosten multipliziert mit der negativen Lernrate erhöht. Der \textit{bias} wird analog erhöht. Die "neuen" Deltas werden durch die Ableitung der Kosten nach dem jeweiligen Ergebnis des Vorgängers berechnet.


\subsection{Speichern \& Laden}
Die Instanzmethode \textit{save(Network, String)} und Klassenmethode \textit{load(String)} in der \textit{NetworkHelper} Klasse, geben dem Benutzer die Optionen das erstellte \textit{Network} zu speichern und dieses auch wieder zu laden.
Um das Netzwerk zu speichern, wird ein neues Dokument mit dem als Parameter übergebenem Namen erstellt. Damit dieses Dokument nun mit dem ausgewählten Netzwerk gefüllt wird, wird von der Klasse \textit{Network} die Methode \textit{save()} aufgerufen. Die Methode ist dafür da um das Netzwerk durchzugehen und seine Werte in das
erstellte Dokument zu schreiben. Für das Laden eines zuvor eingespeicherten Netzwerkes rufen wir die Methode \textit{load()} der \textit{NetzwerkHelper} Klasse auf. Diese Methode bekommt den Namen des gewünschten Netzwerkes als Parameter übergeben damit nun von der \textit{Network} Klasse \textit{load()} aufgerufen werden kann. Als erstes erstellt diese ein neues leeres Netzwerk, um es danach mit den Netzwerk-Daten aus dem übergebenem Dokument zu füllen. Zuletzt wird nur noch das Netzwerk zurück gegeben.


\subsection{toString}
Die \textit{toString()} Methoden dienen der anschaulichen Ausgabe und ggf. Fehlersuche des knN. 

\newpage

\section{Ergebnisse}

\subsection{Begründung der Korrektheit der Umsetzung}
Zur Überprüfen wurden verschiedene Datensätze verwendet, unter anderem die typischen Datensätze von AND-Gatter, OR-Gatter und XOR-Gatter, sowie dem Datensatz eines OR-Gatters mit drei Eingängen:
% Bild 3OR-Gatter
Das Testen unterteilte sich in zwei Abschnitte. Zunächst wurde die Korrektheit des Berechnens überprüft, indem einem vorkonfiguriertem Netz ein Testdatensatz gegeben und die Ausgabe des Netzes mit den zu erwartenden Werten verglichen wurde.

So konnte im zweiten Abschnitt vorausgesetzt werden, dass das Berechnen des Netzes bereits korrekt funktioniert. So wurde ein nicht konfiguriertes Netz mithilfe von Trainingsdatensätzen inklusive deren zu erwartenden Werte trainiert. Anhand der Kosten konnte man hier kontrollieren ob das knN korrekt trainiert wird (Die Kosten müssen dafür mit wenigen Ausnahmen stetig fallend sein). 

Daraus folgend, dass die Tests erfolgreich waren, wurde das Netz als korrekt bewertet.

\subsection{Performance-Überlegungen}
Durch die Iterative Umsetzung der Backpropagation ist eine Aufsummierung der Deltas in einer Layer \textit{L} möglich, sodass erst danach die Layer \textit{L-1} bearbeitet werden kann.
Bei einer Rekursiven Umsetzung müsste man für jedes Output Neuron die Neuronen der restlichen Layers bearbeiten wodurch man jedes Neuron der Hiddenlayers \textit{n} Mal bearbeiten müsste, wobei n die Länge des Outputvektors ist.
Statt einem Array bzw. einer ArrayList verwenden wir in der Backpropagation eine LinkedList um das ständige verschieben der Deltas an Position 1 auf Position 0 im Array zu vermeiden.

\newpage

\section{Diskussion \& Fazit}

\subsection{Vor- \& Nachteile der gewählten Umsetzung}
\subsubsection{Vorteile}
\begin{itemize}
	\item flexibler Umgang mit Aktivierungsfunktionen und deren Ableitung
	\item Flexibilität bei Erstellung des Netzes
	\item durch Auslagerungen von \textit{weights}, \textit{bias}, Aktivierungsfunktion und Ableitung entstehen übersichtlichere Array-Iterationen
\end{itemize}
\subsubsection{Nachteile}
\begin{itemize}
	\item ggf. Fehleranfällig durch flexiblen Umgang mit Aktivierungsfunktionen und deren Ableitung
	\item durch die Benutzung von Function/BiFunction wird ein geringer Verlust von Performance in Kauf genommen
\end{itemize}
\subsection{Was fehlt ? Was könnte erweiternd gemacht werden?}
Das Training für ein Netz, das mindestens eine Hiddenlayer besitzt, ist noch fehlerhaft. Außerdem kann das Programm noch keine Aktivierungsfunktionen und deren Ableitungen speichern.
Dies wäre mit unserer Umsetzung sehr performance-, speicher- und programmieraufwendig. Die Implementierung eines anderen Aktivierungsfunktionssystems, was es erlaubt diese zu speichern und zu laden, könnte ein nächster Schritt sein.
Zudem ist das Speicherformat der \textit{weights} und \textit{biases} nicht auf dem vorgegebenen Standard.
\newpage

\section{GitHub}
Unser Projekt inklusive einer Testklasse und diesem Projektbericht steht auf \textit{\textbf{GitHub}} (\hyperref{https://github.com/Griszder/ProjektSeminar.git}{}{}{\textit{https://github.com/Griszder/ProjektSeminar.git}}) zur Verfügung.

\vspace{1cm}

\section{Anteile am Gesamtprojekt}
Der Projektbericht wurde größtenteils zusammen angefertigt. Der Abschnitt \textit{Methoden} ist in vier Teile aufgeteilt worden. Die Abschnitte 2.2 und 2.3 wurden von Frau Zhang angefertigt, der Abschnitt 2.4 von Frau Zarkh, 2.1, 2.5 und 2.7 hat Herr Schallenberg verfasst und Abschnitt 2.6 schrieb Herr Leggewie. trotz der Kürze des zuletzt genannten Abschnitts, war der Arbeitsaufwand durch die Komplexität des Speicherns und Ladens, der gleiche, wie in den restlichen Abschnitten. Dadurch war die Arbeit am Projektbericht in etwa gleich verteilt. Auch in das Erstellen des knN (zunächst programmiersprachenunabhängig) haben alle ungefähr gleich viel Arbeit investiert. Das Übersetzen des Netzes in die konkrete Programmiersprache Java wurde überwiegend von Herrn Schallenberg stets mithilfe des restlichen Teams, dessen Überlegungen zur Implementierung ebenfalls mit in das Programm einflossen, umgesetzt. Durch einige weitere sehr gute Umsetzungsideen, entstand für Frau Zhang ein zusätzlicher Arbeitsaufwand.

\vspace{1cm}

\section{Verwendete Literatur \& Anhang}
\begin{itemize}
	\hypertarget{1}{\item[]{[1] An Introduction to Neural Networks, Kröse, B., van der Smagt, P., 1996}}
	\hypertarget{2}{\item[]{[2] Neural Networks, 3Blue1Brown, 2018, 
			\hyperref{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{}{}{\textit{https://www.youtube.com/playlist?list\\=PLZHQObOWTQDNU6R1\_67000Dx\_ZCJB-3pi}}}}
	
	
\end{itemize}
\end{document}
