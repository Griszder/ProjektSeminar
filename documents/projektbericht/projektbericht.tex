\documentclass[paper=A4,pagesize=auto,12pt,headinclude=true,footinclude=true,BCOR=0mm,DIV=calc]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{setspace}
\usepackage[left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}
\usepackage[backend=biber, style=ieee, citestyle=ieee]{biblatex}
\onehalfspacing

\bibliography{projektbericht.bib}

%opening
\title{Neuronale Netze}
\author{Alexandra Zarkh, Sui Yin Zhang,\\ Lennart Leggewie, Alexander Schallenberg}

\makeatletter
\def\@maketitle{%
	\newpage
	\null
	\vskip 2em%
	\begin{center}%
		\let \footnote \thanks
		{\Huge \textbf{\@title} \par}%
		\vskip 1em%
		{\Large \textbf{Projektbericht}\par}%
		\vskip 2em%
		{\large
			\lineskip .5em%
			\begin{tabular}[t]{c}%
				\@author
			\end{tabular}\par}%
		\vskip 2em%
		{\large Hochschule Bonn-Rhein-Sieg\par}%
		\vskip 2em%
		{\large \@date}%
	\end{center}%
	\par
	\vskip 1.5em}
\makeatother




\begin{document}

\begin{titlepage}
	\maketitle
\end{titlepage}

\tableofcontents
\newpage



\section{Einleitung}
Ein künstliches neuronales Netz (knN) besteht aus vielen kleinen Verarbeitungseinheiten, die durch Gewichtungen miteinander kommunizieren, wie definiert durch Abschnitt 2.1 in “An Introduction to Neural Networks” \cite{script}.

\subsection{Aufgabenstellung \& Zielsetzung}
Die Aufgabenstellung lautete zunächst, die Grundlagen eines solchen künstlichen neuronalen Netzes in Java zu implementieren, sodass mit diesem schon zu einfachen Eingaben eine korrekte Ausgabe kalkuliert wird (Forward Propagation). Jene weitete sich darauf aus, das Netz trainieren zu können (Backpropagation) und die trainierten Einstellungen des Netzes zu speichern sowie zu laden.\\
Das Ziel war, die erste Hälfte der Aufgabenstellung in den ersten zwei Wochen und die zweite Hälfte in den folgenden zwei Wochen umzusetzen und so ein funktionsfähiges künstliches neuronales Netz in Java zu implementieren.

\subsection{Aufgabenkontext \& externe Vorgaben}
Vorgegeben war, ein knN erstellen zu können, dem man bei seiner Erstellung Gewichtungen sowie Biases übergeben kann. Außerdem soll das Netz Ausgaben abhängig von den Eingaben berechnen können und die Gewichtungen und Biases sollen trainiert werden können. Außerdem ist ein Format zur Abspeicherung der Gewichte und Biases vorgegeben worden, welche in diesem Format in eine CSV-Datei gespeichert werden soll.

\subsection{Nicht vorgegeben aber notwendigerweise von uns festgelegt}
Nicht vorgegeben, aber notwendigerweise festgelegt wurde, dass beim Erstellen eines knN die Anzahl der Neuronen für jede Neuronenschicht vom Benutzer festgelegt wird. Implizit wird damit auch die Anzahl der versteckten Schichten verlangt. Außerdem ist die Struktur des Netzes frei gewählt. Das Programm basiert auf der von uns gewählten Java-Version 17 (openjdk).
% ggf. functions

\subsection{Literaturarbeit: Verweis auf Vorarbeiten}
Die wissenschaftliche Arbeit von Kröse und van der Smagt \cite{script}, welches den Aufbau eines knN beschreibt, sowie Erweiterungen dessen, dient der Definition, dem Grundverständnis und dem groben Aufbau eines solchen Netzes.
Das mathematische Verständnis und die darauf aufbauende Implementierung des knN ergab sich aus den Videos 
der Playlist des YouTube-Kanals 3Blue1Brown \cite{3b1b}.

\newpage


\section{Methoden}
Die Klassenstruktur des Programmes besteht aus den Klassen \textit{Neuron}, \textit{Network}, \textit{NetworkHelper} sowie \textit{ActivationFunction} und davon abgeleitete Klassen und einer zusätzlichen Utility-Klasse \textit{Util}.

\subsection{Util}
Die Klasse \textit{Util} bietet drei abstrakte Hilfsmethoden. \textit{random(int)} gibt ein beliebig-dimensionales Array mit zufälligen Zahlen zurück. \textit{addToVec1(double[], double[])} addiert einen Vektor auf den anderen. Die dritte Methode \textit{mulToVec(double, double[])} multipliziert ein Skalar auf einen Vektor. Jene Methoden erleichtern das Kreieren und den Umgang mit Vektoren bzw. Arrays im knN.


\subsection{Klassenstruktur Network \& Neuron} %Sui Yin
Im \textit{Neuron} sind die \textit{weights}, der \textit{bias}, die Aktivierungsfunktion und eine Hilfsvariable \textit{z} als private Attribute deklariert.
Es befinden sich zwei Konstruktoren in der Klasse, wobei der Erste den Zweiten mit zufälligen \textit{weights} und einem $bias=0$ sowie der Aktivierungsfunktion aufruft und der Zweite dafür zuständig ist, ein genau definiertes Neuron zu kreieren.

In der Klasse \textit{Network} wird das knN erstellt.
Es gibt vier private Attribute, die in den Methoden mehrfach benutzt werden. Das erste private Attribut “inLayerLength” definiert die Länge der “inputLayer”. Die “outputLayer” besteht aus einem eindimensionalen \textit{Neuron}-Array. Dann gibt es noch das Attribut “hiddenLayers”, welches ein zweidimensionales \textit{Neuron}-Array ist, wo der erste Index die Position der jeweiligen Hiddenlayer und der zweite die jeweilige Position des \textit{Neuron}s angibt. Zuletzt gibt es das Attribut “trainable”, welches einen Wahrheitswert ist und standardmäßig wahr ist.

Die Klasse \textit{NetworkHelper} ist dafür da, um Netzwerke zu speichern und zu laden.


\subsection{Konstruktoren \& Initialisierung}
In der Klasse \textit{Network} gibt es insgesamt 6 Konstruktoren.
Diese rufen eine Methode \textit{init} auf um redundanten Code zu verhindern. \textit{init} hat vier Parameter: \textit{int}, \textit{int}, \textit{BiFunction} und ein \textit{int[]}. Die \textit{BiFunction} wird dazu verwendet, der Methode mitzuteilen, wie die Neuronen erstellt werden sollen.
Zuerst werden alle Attribute außer “trainable” entweder direkt mit den passenden Werten oder durch erstellen eines Arrays passender Größe initialisiert.
Darauf folgen zwei Iterationen um diese Arrays mit den übergebenen Werten zu initialisieren. Dabei wird überprüft, ob das knN trainierbar ist, also ob die Ableitung der Aktivierungsfunktion differenzierbar ist. Falls sie es nicht ist, wird dies zwar zugelassen, führt aber dazu, dass das Netz nicht trainierbar ist (trainable wird auf false gesetzt).

Der erste Konstruktor ist dafür da, um ein Netzwerk zu erstellen, wo die Attribute “numInUnit” (die Anzahl der Input Units), ”numOutUnit”(die Anzahl der Output Units), “weights”, ”biases”, sowie die Anzahl der Hiddenlayers gegeben sind. Als Aktivierungsfunktionen \hyperlink{functioning}{(siehe Abschnitt 2.6)} wird hier eine Standardfunktion (Sigmoid) gewählt. Durch die Differenzierbarkeit der Sigmoid-Funktion ist ein solches knN also trainierbar.
Danach folgt der gleiche Konstruktor, mit dem Unterschied, dass die “weights” und die “biases” nicht angegeben sind.
\\
Den nächsten Konstruktoren werden Aktivierungsfunktionen übergeben.
Der dritte und vierte Konstruktor haben einen ähnlichen Aufbau von den Parametern wie der erste und zweite Konstruktor, mit dem Unterschied, dass diesmal eine Aktivierungsfunktion für alle Neuronen übergeben wird.
Statt allen Neuronen die gleiche Aktivierungsfunktion zu geben, bekommen die Neuronen in den letzten beiden Konstruktoren jeweils Eigene.


\subsection{Kalkulation}
Die Methode \textit{compute(double[])} nimmt einen Input-Vektor und gibt einen Output-Vektor zurück. Sie berechnet die Ergebnisse aller Layers mit Hilfe der Methode \textit{forwardPropagation(double[])} und gibt den letzten Ergebnisvektor (Output-Vektor) zurück.

\hypertarget{forwardprop}{\subsubsection{Forward Propagation}}
Die Methode \textit{forwardPropagation(double[])} ist als \textit{private} deklariert und gibt ein zweidimensionales Array des Datentyps \textit{double} zurück. Als Parameter bekommt die Methode einen Input-Vektor.\\
Zuerst wird die tatsächliche Länge des Input-Vektors mit der \textit{inLayerLength} verglichen. Wenn die 
beiden unterschiedlich groß sein sollten, dann wird ein Fehler geworfen. Sind die beiden 
verglichenen Werte jedoch gleich groß, wird ein zweidimensionales Ergebnis-Array angelegt, welches
\textit{results} heißt und ebenfalls vom Datentyp \textit{double} ist. Gleichzeitig wird die
Dimension des Arrays auf die Anzahl der "Layers" (Anzahl \textit{hiddenLayers} + 2) gesetzt. Im ersten Eintrag der \textit{results} wird der Input-Vektor gespeichert. 
Für die nächsten Zeilen des Codes muss vorerst eine weitere Methode erklärt werden. Diese heißt 
\textit{fire(double[])} und ist eine öffentliche Methode die einen \textit{double}-Wert zurückgibt. Als  Eingabeparameter wird ein Input-Vektor übergeben. Zu Anfang der Methode wird eine \textit{double}-Variable \textit{sum} angelegt, die das Ergebnis zwischenspeichert. Der Wert einer Variablen \textit{sum} wird durch die Sigma-Regel
\begin{equation}
	sum = \sum_{i=0}^{n} (w_{i} * in_{i}) + \theta
\end{equation}
\cite{script} berechnet mit \textit{in} = Input-Vektor, \textit{w} = weights-Vektor, \textit{$\theta$} = \textit{bias} und \textit{n} = Länge von \textit{in}. Als Rückgabewert wird die Aktivierung durch die Summe eingesetzt in die Aktivierungsfunktion \(function.apply(sum)\) zurückgegeben.\\
Die nun folgende \textit{for}-Schleife durchläuft die \textit{hiddenLayers} und berechnet mit Hilfe von \textit{fire(double[])} das Ergebnisarray. Die zweite Dimension des Arrays \textit{results} wird unter Verwendung des \textit{hiddenLayers}-Arrays an der Stelle \textit{i} festgelegt. \textit{results} bekommt dann an der Stelle \textit{i+1} ein neues Array der vorher bestimmten Dimension zugewiesen. Nun folgt eine weitere \textit{for}-Schleife mit dem Index \textit{j}, der die zweite Dimension des Arrays \textit{results} mit  \textit{fire(double[])} berechnet. Dabei wird der Fall überprüft, dass wenn \textit{i} gleich \textit{0} sein sollte, der Input-Vektor als erster Eintrag genommen wird und ansonsten, \textit{results} an der Stelle \textit{i} berechnet wird. 
Zuletzt wird die zweite Dimension beim letzten Eintrag der ersten Dimension auf die Länge der
\textit{outputLayer} festgelegt und die Werte der \textit{outputLayer} mit \textit{fire(double[])} berechnet und in \textit{results} abgespeichert. 
Am Ende wird \textit{results} zurückgegeben.


\subsection{Training}
Die Methode \textit{train(double[][], double, double[][], int)} in der Klasse \textit{Network} bietet dem Benutzer die Möglichkeit das knN bzw. dessen \textit{weights} und \textit{biases} zu trainieren, sodass das Netz nach ausreichend Training immer das gewünschte Ergebnis errechnet. Dazu nimmt die Methode mehrere Input-Vektoren und passende Zielvektoren, sowie eine Lernrate und die Anzahl der Wiederholungen des Trainingsprozesses. Auf die Input-Vektoren wird erst die \hyperlink{forwardprop}{Forward Propagation} angewendet um die Ergebnisse der Neuronen zu erhalten, welche dann für den eigentlichen Lernprozess durch die \hyperlink{backprop}{Backpropagation} benutzt werden. Die Kosten des Durchlaufs werden für einen Trainingsdurchlauf summiert und danach zur Trainingskontrolle ausgegeben. Die Methode \textit{cost(double[], double[])} errechnet diese durch
\begin{equation}
	C_0 = \sum_{i=0}^{n} (o_{i} - y_{i})^2
\end{equation}
\cite{3b1b} mit o und y = tatsächliche und gewünschte Ergebnisse und n = Anzahl der Ergebnisse in o.


\hypertarget{backprop}{\subsubsection{Backpropagation}}
Die Umsetzung der Backpropagation ist an die Mathematik von 3Blue1Brown \hyperlink{2}{[2]} angelehnt und beruht auf der Delta-Regel. Zunächst wird eine \textit{LinkedList} angelegt, die der Speicherung der Deltas für die aktuelle Ebene, sowie für die vorherige Ebene dient. Diese Liste hat dadurch immer maximal zwei Elemente. Die Idee ist, das die "neuen" Deltas beim Löschen der “aktuellen” Deltas zu diesen werden.\\Danach werden die ersten Deltas durch die Ableitung der Kostenfunktion ohne Sigma initialisiert. Im nächsten Schritt wird die Backpropagation auf alle Neuronen (innere Schleife) jeder Layer (äußere Schleife), beginnend bei der Output-Layer, angewendet, indem die \textit{backpropagation(double, double, double[])} des jeweiligen Neurons aufgerufen wird und die daraus resultierenden Deltas auf die “neuen” Deltas addiert werden.\\Im Backpropgationteil des Neurons wird jedes Gewicht um den Wert der nach dem Gewicht abgeleiteten Kosten multipliziert mit der negativen Lernrate erhöht. Der \textit{bias} wird analog erhöht. Die “neuen” Deltas werden durch die Ableitung der Kosten nach dem jeweiligen Ergebnis des Vorgängers berechnet.


\hypertarget{functioning}{\subsection{Functioning}}
Das Interface \textit{ActivationFunction} besitzt eine globale Konstante “DEFAULT\_FUNCTION”, die mit dem Wert einer Sigmoid-Funktion initialisiert ist und vier abstrakte Methoden \textit{function(double)}, \textit{derivative(double)}, \textit{toBuffer(BufferedWriter)} und \textit{fromBuffer(Scanner)}, welche eine mathematische Funktion sowie deren Ableitung repräsentieren soll und dem Speichern und Laden einer Aktivierungsfunktion dienen. Ein JavaDoc Kommentar weißt darauf hin, dass jede von diesem Interface abgeleitete Klasse einen Konstruktor besitzen muss, welcher keine Parameter nimmt, damit die von der Klasse dargestellte Aktivierungsfunktion korrekt geladen werden kann. \\
Zudem gibt es die vorimplementierten Aktivierungsfunktionen \textit{Identity} (Identitätsfunktion), \textit{Logistic} (logistische Funktion), \textit{SemiLinear} (Semi-lineare Funktion), \textit{Sigmoid} (Sigmoid-Funktion), \textit{Sign} (Sign-Funktion), \textit{SignDiff} (differenzierbare Sign-Funktion, mathematisch inkorrekt) und \textit{TangensHyperbolicus} (Tangens hyperbolicus).


\subsection{Speichern \& Laden}
Die Klassenmethoden \textit{save(Network, String)} und \textit{load(String)} in der Klasse \textit{NetworkHelper}, geben dem Benutzer die Option das erstellte knN zu speichern und dieses auch wieder zu laden. Um es zu speichern, wird eine neue CSV-Datei mit dem als Parameter übergebenem Namen erstellt. Damit diese Datei nun auch mit dem übergebenen Netz gefüllt wird, wird in der jeweiligen Instanz der Klasse \textit{Network} die Methode \textit{save(BufferedWriter)} aufgerufen. Diese ist dafür da um das knN durchzugehen und seine \textit{weights} und \textit{biases} im vorgegebenen Format (also mit Semikola getrennt) in die erstellte CSV-Datei zu schreiben. Um ein zuvor gespeichertes Netz zu laden, wird die Methode \textit{load(String)} der Klasse \textit{NetworkHelper} aufgerufen, welche den Dateinamen der zu ladenden Datei fordert. Nun werden die Werte aus der gewünschten Datei gelesen und in temporäre Variablen gespeichert. Danach wird mit ihnen das entsprechende knN wieder aufgebaut und anschließend zurückgegeben.
\\
In bzw. aus einer separaten CSV-Datei werden mithilfe der Methoden \textit{saveFunctions(Network, String)} und \textit{loadFunctions(String)} der Klasse \textit{NetworkHelper} die einzelnen Aktivierungsfunktionen der Neuronen des knN gespeichert bzw. geladen. Auch hierbei wird das Speichern der Funktionen an das Netz übergeben, weswegen die Klasse \textit{Network} eine package-pivate Instanzmethode saveFunctions(String) besitzt.


\subsection{toString}
Die \textit{toString()} Methoden dienen der anschaulichen Ausgabe und ggf. Fehlersuche des knN. 

\vspace{2cm}

\section{Ergebnisse}
Das in der Einleitung genannte Ziel, ein funktionsfähiges künstliches neuronales Netz in Java zu implementieren, wurde dieses erreicht. Dies ist begründet durch ausgiebiges Testen.
\subsection{Begründung der Korrektheit der Umsetzung}
Zur Überprüfen wurden verschiedene Datensätze verwendet, unter anderem die typischen Datensätze von AND-Gatter, OR-Gatter und XOR-Gatter, sowie dem Datensatz eines OR-Gatters mit drei Eingängen:
\begin{center}
\begin{tabular}{|c|c|c|}
	\hline
	\multicolumn{3}{|c|}{\textbf{AND-Gate}} \\ \hline
	\hline
	\multicolumn{2}{|c|}{\textbf{Input}} & \textbf{Output} \\ \hline
	$x_0$ & $x_1$ & $y$ \\
	\hline\hline
	0 & 0 & 0 \\ \hline
	0 & 1 & 0 \\ \hline
	1 & 0 & 0 \\ \hline
	1 & 1 & 1 \\ \hline
\end{tabular}
\begin{tabular}{|c|c|c|}
	\hline
	\multicolumn{3}{|c|}{\textbf{OR-Gate}} \\ \hline
	\hline
	\multicolumn{2}{|c|}{\textbf{Input}} & \textbf{Output} \\ \hline
	$x_0$ & $x_1$ & $y$ \\
	\hline\hline
	0 & 0 & 0 \\ \hline
	0 & 1 & 1 \\ \hline
	1 & 0 & 1 \\ \hline
	1 & 1 & 1 \\ \hline
\end{tabular}
\begin{tabular}{|c|c|c|}
	\hline
	\multicolumn{3}{|c|}{\textbf{XOR-Gate}} \\ \hline
	\hline
	\multicolumn{2}{|c|}{\textbf{Input}} & \textbf{Output} \\ \hline
	$x_0$ & $x_1$ & $y$ \\
	\hline\hline
	0 & 0 & 0 \\ \hline
	0 & 1 & 1 \\ \hline
	1 & 0 & 1 \\ \hline
	1 & 1 & 0 \\ \hline
\end{tabular}
\begin{tabular}{|c|c|c|c|}
	\hline
	\multicolumn{4}{|c|}{\textbf{3OR-Gate}} \\ \hline
	\hline
	\multicolumn{3}{|c|}{\textbf{Input}} & \textbf{Output} \\ \hline
	$x_0$ & $x_1$ & $x_2$ & $y$ \\
	\hline\hline
	0 & 0 & 0 & 0 \\ \hline
	0 & 0 & 1 & 1 \\ \hline
	0 & 1 & 0 & 1 \\ \hline
	0 & 1 & 1 & 1 \\ \hline
	1 & 0 & 0 & 1 \\ \hline
	1 & 0 & 1 & 1 \\ \hline
	1 & 1 & 0 & 1 \\ \hline
	1 & 1 & 1 & 1 \\ \hline
\end{tabular}
\end{center}
Das Testen unterteilte sich in zwei Abschnitte. Zunächst wurde die Korrektheit des Berechnens überprüft, indem einem vorkonfiguriertem Netz ein Testdatensatz gegeben und die Ausgabe des Netzes mit den zu erwartenden Werten verglichen wurde.

So konnte im zweiten Abschnitt vorausgesetzt werden, dass das Berechnen des Netzes bereits korrekt funktioniert. So wurde ein nicht konfiguriertes Netz mithilfe von Trainingsdatensätzen inklusive deren zu erwartenden Werte trainiert. Anhand der Kosten konnte man hier kontrollieren ob das knN korrekt trainiert wird (Die Kosten müssen dafür mit wenigen Ausnahmen stetig fallend sein). 

Daraus folgend, dass die Tests erfolgreich waren, wurde das Netz als korrekt bewertet.

\subsection{Performance-Überlegungen}
Durch die Iterative Umsetzung der Backpropagation ist eine Aufsummierung der Deltas in einer Layer \textit{L} möglich, sodass erst danach die Layer \textit{L-1} bearbeitet werden kann. Bei einer Rekursiven Umsetzung müsste man für jedes Output Neuron die Neuronen der restlichen Layers bearbeiten wodurch man jedes Neuron der Hiddenlayers $n$ Mal bearbeiten müsste, wobei $n$ die Länge des Outputvektors ist. Daraus ergibt sich bei der iterativen Umsetzung eine lineare Laufzeit, abhängig von Größe und Anzahl der vorangehenden Layers, und bei der rekursiven Umsetzung eine quadratische Laufzeit abhängig von eben genanntem Faktor sowie Größe der aktuellen Layer.
Statt einem Array bzw. einer ArrayList verwenden wir in der Backpropagation eine LinkedList um das ständige verschieben der Deltas an Position 1 auf Position 0 im Array zu vermeiden.

Für forward propagation und backpropagation fällt jeweils eine kubische Laufzeit an, welches durch die Gewichtungen (pro Zwischenraum von Layers eine Gewichtsmatrix) des Netzes begründet ist.

Wenn man Wiederholungs- und Datensatziterationsschleife in der Methode \textit{train(double[][], double[][], double, int)} berücksichtigt, ist dort sogar eine Laufzeit von $n^5$ vorhanden.

Neben dem eigentlichen Arbeiten mit externen Datensätzen, ist auch die Performance von Laden  \& Speichern zu berücksichtigen. Beides hat eine kubische Laufzeit, wobei beim Laden des knN durch die Benutzung von \textit{Reflection} eine lineare Laufzeit mit relativ hohem Vorfaktor zu finden ist, was das gerade Speichern von kleineren Netzes verlangsamt.

Nicht zu vergessen ist die Initialisierung des kNN. Da in dieser jedes Neuron jeder Layer erstellt wird, handelt es sich hierbei um eine quadratische Laufzeit.



\newpage

\section{Diskussion \& Fazit}
Dass das Ziel erreicht ist, impliziert nicht, dass die Umsetzung perfekt ist. In Bereichen wie Performance, Speicheraufwand und Benutzerfreundlichkeit sind Vor- sowie Nachteile zu finden und können stetig verbessert werden.

\subsection{Vorteile der gewählten Umsetzung}
Durch die gewählte Aktivierungsfunktionsstruktur, ist ein flexibler Umgang mit diesen möglich, sodass auch Benutzer des Netzes je nach Anwendung eigene Aktivierungsfunktionen implementieren und im Netz benutzen können.
Eine Vielzahl von Konstruktoren bietet viele verschiedene Möglichkeiten ein künstliches neuronales Netz zu erstellen. So muss ein komplexer Konstruktor nur bei komplexer Konfiguration des Netzes verwendet werden.
Dies macht die Umsetzung benutzerfreundlich.
Die Auslagerungen von \textit{weights}, \textit{bias}, Aktivierungsfunktion und Ableitung führen zu übersichtlicheren Array-Iterationen und damit zu einem gut lesbaren Code.

\subsection{Nachteile der gewählten Umsetzung}
Ein Nachteil der Implementierung ist, dass jede Aktivierungsfunktion ein Konstruktor besitzen muss, der keine Übergabeparameter nimmt, um das korrekte Laden dieser Funktionen aus einer Datei in ein knN garantieren zu können. Außerdem müssen die Methoden \textit{toBuffer(BufferedWriter)} und \textit{fromBuffer(Scanner)} aufeinander abgestimmt sein, damit kein Fehler beim Laden entsteht.
Wie in Abschnitt 3.2 bereits genannt zieht auch die Benutzung von \textit{Reflection} den Nachteil mit sich, dass es sich hierbei um einen relativ großen Performance-Aufwand handelt.

\subsection{Was fehlt ? Was könnte erweiternd gemacht werden?}
Da das Netz funktioniert und allen Vorgaben entspricht ist die Implementation vollständig. Verbesserungen können in Bereichen wie Performance, Speicheraufwand und Benutzerfreundlichkeit ständig vorgenommen werden. Außerdem könnte das Testen um Anwendungsfälle außerhalb der logischen Gatter erweitert werden (z.B. Mnist). Es können Erweiterungen des Netzes (z.B. die eigenständige Berechnung einer angemessenen Lernrate) implementiert werden.
Ausreichend JavaDoc-Kommentare im Code würden die Verständlichkeit dessen unterstreichen.
\newpage

\section{GitHub}
Unser Projekt inklusive einer Testklasse und dieses Projektberichts steht auf \textbf{GitHub} (\hyperref{https://github.com/Griszder/ProjektSeminar.git}{}{}{\textit{https://github.com/Griszder/ProjektSeminar.git}}) zur Verfügung.

\vspace{1cm}

\section{Anteile am Gesamtprojekt}
Der Projektbericht wurde größtenteils zusammen angefertigt. Der Abschnitt \textit{Methoden} ist in vier Teile aufgeteilt worden. Die Abschnitte 2.2 und 2.3 wurden von Frau Zhang angefertigt, der Abschnitt 2.4 von Frau Zarkh, 2.1, 2.5 und 2.8 hat Herr Schallenberg verfasst und Abschnitt 2.7 schrieb Herr Leggewie. trotz der Kürze des zuletzt genannten Abschnitts, war der Arbeitsaufwand durch die Komplexität des Speicherns und Ladens, der gleiche, wie in den restlichen Abschnitten. Der Abschnitt 2.6 ist gemeinsam verfasst worden. Dadurch war die Arbeit am Projektbericht in etwa gleich verteilt. Auch in das Erstellen des knN (zunächst programmiersprachenunabhängig) haben alle ungefähr gleich viel Arbeit investiert. Das Übersetzen des Netzes in die konkrete Programmiersprache Java wurde überwiegend von Herrn Schallenberg stets mithilfe des restlichen Teams, dessen Überlegungen zur Implementierung ebenfalls mit in das Programm einflossen, umgesetzt. Durch einige weitere sehr gute Umsetzungsideen, entstand für Frau Zhang ein zusätzlicher Arbeitsaufwand.

\vspace{1cm}

\printbibliography[heading=bibnumbered]
\end{document}
