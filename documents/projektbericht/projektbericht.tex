\documentclass[paper=A4,pagesize=auto,12pt,headinclude=true,footinclude=true,BCOR=0mm,DIV=calc]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{setspace}
\usepackage[left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}
\onehalfspacing


%opening
\title{Neuronale Netze}
\author{Alexandra Zarkh, Sui Yin Zhang,\\ Lennart Leggewie, Alexander Schallenberg}

\makeatletter
\def\@maketitle{%
	\newpage
	\null
	\vskip 2em%
	\begin{center}%
		\let \footnote \thanks
		{\Huge \textbf{\@title} \par}%
		\vskip 1em%
		{\Large \textbf{Projektbericht}\par}%
		\vskip 2em%
		{\large
			\lineskip .5em%
			\begin{tabular}[t]{c}%
				\@author
			\end{tabular}\par}%
		\vskip 2em%
		{\large Hochschule Bonn-Rhein-Sieg\par}%
		\vskip 2em%
		{\large \@date}%
	\end{center}%
	\par
	\vskip 1.5em}
\makeatother




\begin{document}

\begin{titlepage}
	\maketitle
\end{titlepage}

\tableofcontents
\newpage



\section{Einleitung}

\subsection{Aufgabenstellung \& Zielsetzung}
Die Aufgabenstellung lautete zunächst, die Grundlagen eines künstlichen neuronalen Netzes (knN) in Java zu implementieren, sodass mit diesem schon zu einfachen Eingaben eine korrekte Ausgabe kalkuliert wird (Forward Propagation). Diese weiteten sich darauf aus, das Netz trainieren zu können (Backpropagation) und die trainierten Einstellungen des Netzes zu speichern und zu laden.\\
Das Ziel war, die erste Hälfte der Aufgabenstellung in den ersten zwei Wochen und die zweite Hälfte in den folgenden zwei Wochen umzusetzen.

\subsection{Aufgabenkontext \& externe Vorgaben}
Vorgegeben war, ein knN erstellen zu können, dem man bei seiner Erstellung Gewichtungen sowie Biases übergeben kann. Außerdem soll das Netz Ausgaben abhängig von den Eingaben berechnen können und die Gewichtungen und Biases sollen trainiert werden können. Außerdem ist ein Format zur Abspeicherung der Gewichte und Biases vorgegeben worden, welche in diesem Format in eine CSV-Datei gespeichert werden soll.

\subsection{Nicht vorgegeben aber notwendigerweise von uns festgelegt}
Nicht vorgegeben, aber notwendigerweise festgelegt wurde, dass beim Erstellen eines knN die Anzahl der Neuronen für jede Neuronenschicht vom Benutzer festgelegt wird. Implizit wird damit auch die Anzahl der versteckten Schichten verlangt. Außerdem ist die Struktur des Netzes frei gewählt.
% ggf. functions

\subsection{Literaturarbeit: Verweis auf Vorarbeiten}
\begin{itemize}
	\item{An Introduction to Neural Networks, Kroese, B., a Van der Smagt, P., 1996}
	\hypertarget{3b1b}{\item{Neural Networks, 3Blue1Brown, 2018, \hyperref{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{}{}{YouTube}}}
\end{itemize}


\newpage


\section{Methoden}
% Beschreibung \& Begründung der Umsetzung
% Beschreibung der Feinstrukturen (Programmteile, Objekte/Klassen und besondere Herausforderungen)

\subsection{Util}
Die Klasse \textit{Util} bietet die abstrakten Hilfsmethoden \textit{random(int)}, \textit{addToVec1(double[], double[])} und \textit{mulToVec(double, double[])} für das kreieren und für den Umgang mit Vektoren bzw. Arrays. Da diese Methoden mit ausreichend JavaDoc ausgestattet sind, wodurch sie selbsterklärend sind, wird auf diese hier nicht näher eingegangen.


\subsection{Klassenstruktur Network \& Neuron} %Sui Yin
Die Klassenstruktur des Programmes besteht neben \textit{Util} aus den Klassen \textit{Neuron}, \textit{Network} sowie \textit{NetworkHelper}.

Im \textit{Neuron} sind die \textit{weights}, der \textit{bias}, die Aktivierungsfunktion sowie deren Ableitung und eine Hilfsvariable \textit{z} als private Attribute definiert.
Es befinden sich zwei Konstruktoren in der Klasse, wobei der Erste den Zweiten mit zufälligen \textit{weights} und einem \textit{bias}=0 sowie der Funktion und deren Abletiung aufruft und der Zweite dafür zuständig ist, einen genau definiertes Neuron zu kreieren.

In der Klasse \textit{Network} wird das knN erstellt.
Es befinden sich zwei Konstanten in der Klasse, die die mathematischen Formeln für die Sigmoidfunktion und deren Ableitung darstellt. Außerdem gibt es noch vier private Attribute,  die in den Methoden mehrfach benutzt werden. Das erste private Attribut “inLayerLength” definiert die Länge der input layer. Die “outputLayer” besteht aus einem eindimensonalen \textit{Neuron}-Array. Dann gibt es noch das Attribut “hiddenLayers”, welches ein zweidimensonales \textit{Neuron}-Array ist, wo der erste Index die Position der jeweiligen Hiddenlayer und der zweite die jeweilige Position des \textit{Neuron}s angibt. Zuletzt gibt es das Attribut “trainable”, welches einen Wahrheitswert ist und standartmäßig wahr ist.

Die \textit{NetworkHelper} Klasse ist dafür da, um Netzwerke zu speichern und zu laden.


\subsection{Konstruktoren \& Initialisierung}
In der Klasse Network gibt es insgesamt 7 Konstruktoren.
Alle außer dem ersten Konstruktor rufen eine “init”-Methode auf um redudanten Code zu verhindern. Die "init"-Methode hat vier Parameter: \textit{int}, \textit{int}, \textit{BiFunction} und ein \textit{int[]}. Die \textit{BiFunction} wird dazu verwendet, der Methode mitzuteilen, wie die Neuronen erstellt werden sollen.
Zuerst werden alle Attribute außer "trainable" entweder direkt mit den passenden Werten oder durch erstellen eines Arrays passender Größe initialisiert.
Darauf folgen zwei Iterationen um diese Arrays mit den übergebenen Werten zu initialisieren.

Der erste Konstruktor ist ein privater leerer Konstruktor für das Laden eines Netzes. 

Der zweite Konstruktor ist dafür da, um ein Netzwerk zu erstellen, wo die Attribute “numInUnit” (die Anzahl der Input Units),”numOutUnit”(die Anzahl der Output Units,“weights”,”biases”, sowie die Anzahl der Hiddenlayers vorgegeben sind.
-> kann gar nicht sein, dass es nicht trainierbar ist, da es default function immer ableitbar ist.

Danach folgt der gleiche Konstruktor, mit dem Unterschied, dass die “weights” und die “biases” nicht angegeben sind.

In den nächsten Konstruktoren, die Aktivierungsfunktionen und deren Ableitungen übergeben bekommen, wird immer erst überprüft, ob das knN trainierbar ist, also ob die Ableitungsfunktion nicht null ist. Falls sie es doch ist, wird dies zwar zugelassen, führt aber dazu, dass das Netz nicht trainierbar ist. 

Der vierte und fünfte Konstruktor haben einen ähnlichen Aufbau von den Parametern wie der zweite und dritte Konstruktor, mit dem Unterschied, dass diesmal eine Funktion, mit dessen Ableitung übergeben wird.

Statt allen Neuronen die gleiche Aktivierungsfunktion und Ableitung zu geben, bekommen die Neuronen in den letzten beiden Konstruktoren jeweils Eigene.


\subsection{Kalkulation}
Die Methode \textit{compute(double[])} nimmt einen Input-Vektor und gibt einen Output-Vektor zurück. Sie berechnet die Ergebnisse aller Layers mit Hilfe der Methode \textit{forwardPropagation(double[])} und gibt den letzten Ergebnisvektor (Output-Vektor) zurück.

\hypertarget{forwardprop}{\subsubsection{Forward Propagation}}
Die Methode \textit{forwardPropagation(double[])} ist als \textit{private} deklariert und gibt ein zweidimensionales Array des Datentyps \textit{double} zurück. Als Parameter bekommt die Methode einen Input-Vektor.\\
Zuerst wird die tatsächliche Länge des Input-Vektors mit der \textit{inLayerLength} verglichen. Wenn die 
beiden unterschiedlich groß sein sollten, dann wird ein Fehler geworfen. Sind die beiden 
verglichenen Werte jedoch gleich groß, wird ein zweidimensionales Ergebnis-Array angelegt, welches
\textit{results} heißt und ebenfalls vom Datentyp \textit{double} ist. Gleichzeitig wird die
Dimension des Arrays auf die Anzahl der "Layers" (Anzahl \textit{hiddenLayers} + 2) gesetzt. Im ersten Eintrag der \textit{results} wird der Input-Vektor gespeichert. 
Für die nächsten Zeilen des Codes muss vorerst eine weitere Methode erklärt werden. Diese heißt 
\textit{fire(double[])} und ist eine öffentliche Methode die einen \textit{double}-Wert zurückgibt. Als  Eingabeparameter wird ein Input-Vektor übergeben. Zu Anfang der Methode wird eine \textit{double}-Variable \textit{sum} angelegt, die das Ergebnis zwischenspeichert. Der Wert einer Variablen \textit{sum} wird durch die Sigma-Regel
\begin{equation}
	sum = \sum_{i=0}^{n} (w_{i} * in_{i}) + \theta
\end{equation}
berechnet mit \textit{in} = Input-Vektor, \textit{w} = weights-Vektor, \textit{$\theta$} = \textit{bias} und \textit{n} = Länge von \textit{in}. Als Rückgabewert wird die Aktivierung durch die Summe eingesetzt in die Aktivierungsfunktion \(function.apply(sum)\) zurückgegeben.\\
Die nun folgende \textit{for}-Schleife durchläuft die \textit{hiddenLayers} und berechnet mit Hilfe von \textit{fire(double[])} das Ergebnisarray. Die zweite Dimension des Arrays \textit{results} wird unter Verwendung des \textit{hiddenLayers}-Arrays an der Stelle \textit{i} festgelegt. \textit{results} bekommt dann an der Stelle \textit{i+1} ein neues Array der vorher bestimmten Dimension zugewiesen. Nun folgt eine weitere \textit{for}-Schleife mit dem Index \textit{j}, der die zweite Dimension des Arrays \textit{results} mit  \textit{fire(double[])} berechnet. Dabei wird der Fall überprüft, dass wenn \textit{i} gleich \textit{0} sein sollte, der Input-Vektor als erster Eintrag genommen wird und ansonsten, \textit{results} an der Stelle \textit{i} berechnet wird. 
Zuletzt wird die zweite Dimension beim letzten Eintrag der ersten Dimension auf die Länge der
\textit{outputLayer} festgelegt und die Werte der \textit{outputLayer} mit \textit{fire(double[])} berechnet und in \textit{results} abgespeichert. 
Am Ende wird \textit{results} zurückgegeben.


\subsection{Training}
Die Methode \textit{train(double[][], double, double[][], int)} in der Klasse \textit{Network} bietet dem Benutzer die Möglichkeit das knN bzw. dessen \textit{weights} und \textit{biases} zu trainieren, sodass das Netz nach ausreichend Training immer das gewünschte Ergebnis errechnet. Dazu nimmt die Methode mehrere Input-Vektoren und passende Zielvektoren, sowie eine Lernrate und die Anzahl der Wiederholungen des Trainingsprozesses. Auf die Input-Vektoren wird erst die \hyperlink{forwardprop}{Forward Propagation} angewendet um die Ergebnisse der Neuronen zu erhalten, welche dann für den eigentlichen Lernprozess durch die \hyperlink{backprop}{Backpropagation} benutzt werden. Die Kosten des Durchlaufs werden für einen Trainingsdurchlauf summiert und danach zur Trainingskontrolle ausgegeben. Die Methode \textit{cost(double[], double[])} errechnet diese durch
\begin{equation}
	C_0 = \sum_{i=0}^{n} (o_{i} - y_{i})^2
\end{equation}
mit o und y = tatsächliche und gewünschte Ergebnisse und n = Anzahl der Ergebnisse in o.


\hypertarget{backprop}{\subsubsection{Backpropagation}}
Die Umsetzung der Backpropagation ist an die Mathematik von \hyperlink{3b1b}{3Blue1Brown} angelehnt und beruht auf der Delta-Regel. Zunächst wird eine \textit{LinkedList} angelegt, die der Speicherung der Deltas für die aktuelle Ebene, sowie für die vorherige Ebene dient. Diese Liste hat dadurch immer maximal zwei Elemente. Die Idee ist, das die "neuen" Deltas beim Löschen der "aktuellen" Deltas zu diesen werden.\\Danach werden die ersten Deltas durch die Ableitung der Kostenfunktion ohne Sigma initialisiert. Im nächsten Schritt wird die Backpropagation auf alle Neuronen (innere Schleife) jeder Layer (äußere Schleife), beginnend bei der Output-Layer, angewendet, indem die \textit{backpropagation(double, double, double[])} des jeweiligen Neurons aufgerufen wird und die daraus resultierenden Deltas auf die "neuen" Deltas addiert werden.\\Im Backpropgationteil des Neurons wird jedes Gewicht um den Wert der nach dem Gewicht abgeleiteten Kosten multipliziert mit der negativen Lernrate erhöht. Der \textit{bias} wird analog erhöht. Die "neuen" Deltas werden durch die Ableitung der Kosten nach dem jeweiligen Ergebnis des Vorgängers berechnet.


\subsection{Speichern \& Laden}
Die Instanzmethode \textit{save(Network, String)} und Klassenmethode \textit{load(String)} in der \textit{NetworkHelper} Klasse, geben dem Benutzer die Optionen das erstellte \textit{Network} zu speichern und dieses auch wieder zu laden.
Um das Netzwerk zu speichern, wird ein neues Dokument mit dem als Parameter übergebenem Namen erstellt. Damit dieses Dokument nun mit dem ausgewählten Netzwerk gefüllt wird, wird von der Klasse \textit{Network} die Methode \textit{save()} aufgerufen. Die Methode ist dafür da um das Netzwerk durchzugehen und seine Werte in das
erstellte Dokument zu schreiben. Für das Laden eines zuvor eingespeicherten Netzwerkes rufen wir die Methode \textit{load()} der \textit{NetzwerkHelper} Klasse auf. Diese Methode bekommt den Namen des gewünschten Netzwerkes als Parameter übergeben damit nun von der \textit{Network} Klasse \textit{load()} aufgerufen werden kann. Als erstes erstellt diese ein neues leeres Netzwerk, um es danach mit den Netzwerk-Daten aus dem übergebenem Dokument zu füllen. Zuletzt wird nur noch das Netzwerk zurück gegeben.


\subsection{toString}
Die \textit{toString()} Methoden dienen der anschaulichen Ausgabe und ggf. Fehlersuche des knN. 

\newpage

\section{Ergebnisse}

\subsection{Begründung der Korrektheit der Umsetzung}
Neben der Umsetzung mathematisch korrekter Formeln stellten wir durch Testen mit verschiedenen logischen Gattern die Korrektheit der Berechnung der Ergebnisse \(compute (double[])\) fest.
Außerdem stellten wir durch dieses Testen fest, dass das Training in einem Netz ohne Hiddenlayers ebenfalls funktoniert.

\subsection{Performance-Überlegungen}
Durch die Iterative Umsetzung der Backpropagation ist eine Aufsummierung der Deltas in einer Layer \textit{L} möglich, sodass erst danach die Layer \textit{L-1} bearbeitet werden kann.
Bei einer Rekursiven Umsetzung müsste man für jedes Output Neuron die Neuronen der restlichen Layers bearbeiten wodurch man jedes Neuron der Hiddenlayers \textit{n} Mal bearbeiten müsste, wobei n die Länge des Outputvektors ist.
Statt einem Array bzw. einer ArrayList verwenden wir in der Backpropagation eine LinkedList um das ständige verschieben der Deltas an Position 1 auf Position 0 im Array zu vermeiden.

\newpage

\section{Diskussion \& Fazit}

\subsection{Vor- \& Nachteile der gewählten Umsetzung}
\subsubsection{Vorteile}
\begin{itemize}
	\item flexibler Umgang mit Aktivierungsfunktionen und deren Ableitung
	\item Flexiblität bei Erstellung des Netzes
	\item durch Auslagerungen von \textit{weights}, \textit{bias}, Aktivierungsfunktion und Ableitung entstehen übersichtlichere Array-Iterationen
\end{itemize}
\subsubsection{Nachteile}
\begin{itemize}
	\item ggf. Fehleranfällig durch flexiblen Umgang mit Aktivierungsfunktionen und deren Ableitung
	\item durch die Benutzung von Function/BiFunction wird ein geringer Verlust von Performance in Kauf genommen
\end{itemize}
\subsection{Was fehlt ? Was könnte erweiternd gemacht werden?}
Das Training für ein Netz, das mindestenst eine Hiddenlayer besitzt, ist noch fehlerhaft. Außerdem kann das Programm noch keine Aktivierungsfunktionen und deren Ableitungen speichern.
Dies wäre mit unserer Umsetzung sehr performance-, speicher- und programmieraufwendig. Die Implentierung eines anderen Aktivierungsfunktionssystems, was es erlaubt diese zu speichern und zu laden, könnte ein nächster Schritt sein.
Zudem ist das Speicherformat der \textit{weights} und \textit{biases} nicht auf dem vorgegebenen Standard.
\newpage

\section{Verwendete Literatur \& Anhang}
\begin{itemize}
	\item{An Introduction to Neural Networks, Kroese, B., a Van der Smagt, P., 1996}
	\hypertarget{3b1b}{\item{Neural Networks, 3Blue1Brown, 2018, \hyperref{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{}{}{YouTube}}}
\end{itemize}

\vspace{1cm}

\section{GitHub}
Unser Projekt inklusive einer Testklasse und diesem Projektbericht steht auf \hyperref{https://github.com/Griszder/ProjektSeminar.git}{}{}{\textit{\textbf{GitHub}}} zur Verfügung.

\end{document}
